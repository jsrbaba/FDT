{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "data = pd.read_csv('data33.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data.drop([\"PATIENT_UPDATE\"], axis = 1)\n",
    "Y = data[\"PATIENT_UPDATE\"]\n",
    "\n",
    "X_train,X_test,y_train,y_test= train_test_split(X,Y,test_size=0.3)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "def print_confusionMatrix(Y_TestLabels, PredictedLabels):\n",
    "    confusionMatx = confusion_matrix(Y_TestLabels, PredictedLabels)\n",
    "    \n",
    "    precision = confusionMatx/confusionMatx.sum(axis = 0)\n",
    "    \n",
    "    recall = (confusionMatx.T/confusionMatx.sum(axis = 1)).T\n",
    "    \n",
    "    sns.set(font_scale=1.5)\n",
    "    \n",
    "    # confusionMatx = [[1, 2],\n",
    "    #                  [3, 4]]\n",
    "    # confusionMatx.T = [[1, 3],\n",
    "    #                   [2, 4]]\n",
    "    # confusionMatx.sum(axis = 1)  axis=0 corresponds to columns and axis=1 corresponds to rows in two diamensional array\n",
    "    # confusionMatx.sum(axix =1) = [[3, 7]]\n",
    "    # (confusionMatx.T)/(confusionMatx.sum(axis=1)) = [[1/3, 3/7]\n",
    "    #                                                  [2/3, 4/7]]\n",
    "\n",
    "    # (confusionMatx.T)/(confusionMatx.sum(axis=1)).T = [[1/3, 2/3]\n",
    "    #                                                    [3/7, 4/7]]\n",
    "    # sum of row elements = 1\n",
    "    \n",
    "    labels = [\"0=No SEVER \",\"1=SEVER \"]\n",
    "    \n",
    "    plt.figure(figsize=(16,7))\n",
    "    sns.heatmap(confusionMatx, cmap = \"YlGnBu_r\", annot = True, fmt = \".1f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(\"Confusion Matrix\", fontsize = 30)\n",
    "    plt.xlabel('Predicted Class', fontsize = 20)\n",
    "    plt.ylabel('Original Class', fontsize = 20)\n",
    "    plt.tick_params(labelsize = 15)\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    print(\"-\"*125)\n",
    "    \n",
    "    plt.figure(figsize=(16,7))\n",
    "    sns.heatmap(precision, cmap = \"YlGnBu_r\", annot = True, fmt = \".2f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(\"Precision Matrix\", fontsize = 30)\n",
    "    plt.xlabel('Predicted Class', fontsize = 20)\n",
    "    plt.ylabel('Original Class', fontsize = 20)\n",
    "    plt.tick_params(labelsize = 15)\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    print(\"-\"*125)\n",
    "    \n",
    "    plt.figure(figsize=(16,7))\n",
    "    sns.heatmap(recall, cmap = \"YlGnBu_r\", annot = True, fmt = \".2f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(\"Recall Matrix\", fontsize = 30)\n",
    "    plt.xlabel('Predicted Class', fontsize = 20)\n",
    "    plt.ylabel('Original Class', fontsize = 20)\n",
    "    plt.tick_params(labelsize = 15)\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    sensitivity = confusionMatx[0,0]/(confusionMatx[0,0]+confusionMatx[1,0])\n",
    "    print('Sensitivity : ', sensitivity )\n",
    "\n",
    "    specificity = confusionMatx[1,1]/(confusionMatx[1,1]+confusionMatx[0,1])\n",
    "    print('Specificity : ', specificity)\n",
    "    \n",
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    plt.plot(fpr, tpr, color='darkorange',lw=lw, \n",
    "             label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0,1], [0,1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0, 1.0])\n",
    "    plt.ylim([0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    #plt.title('')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()  \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "precision, recall, f1 = precision_recall_fscore_support(y_test,y_pred,average='macro')[:-1]\n",
    "print(\"accuracy: \", accuracy)\n",
    "print(\"precision: \",precision)\n",
    "print(\"recall: \",recall)\n",
    "print(\"f1: \",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_confusionMatrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_score = clf.predict_proba(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score[:,1])\n",
    "plot_roc_curve(fpr, tpr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'y_test':y_test,'y_score[:,1]': y_score[:,1]})\n",
    "df1.to_csv('Pictures/pre_lab_5.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier().fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "precision, recall, f1 = precision_recall_fscore_support(y_test,y_pred,average='macro')[:-1]\n",
    "print(\"accuracy: \", accuracy)\n",
    "print(\"precision: \",precision)\n",
    "print(\"recall: \",recall)\n",
    "print(\"f1: \",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_confusionMatrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_score = clf.predict_proba(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score[:,1])\n",
    "plot_roc_curve(fpr, tpr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'y_test':y_test,'y_score[:,1]': y_score[:,1]})\n",
    "df1.to_csv('Pictures/pre_lab_6.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB().fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "precision, recall, f1 = precision_recall_fscore_support(y_test,y_pred,average='macro')[:-1]\n",
    "print(\"accuracy: \", accuracy)\n",
    "print(\"precision: \",precision)\n",
    "print(\"recall: \",recall)\n",
    "print(\"f1: \",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_confusionMatrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_score = clf.predict_proba(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score[:,1])\n",
    "plot_roc_curve(fpr, tpr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'y_test':y_test,'y_score[:,1]': y_score[:,1]})\n",
    "df1.to_csv('Pictures/pre_lab_7.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(max_depth=3)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "precision, recall, f1 = precision_recall_fscore_support(y_test,y_pred,average='macro')[:-1]\n",
    "print(\"accuracy: \", accuracy)\n",
    "print(\"precision: \",precision)\n",
    "print(\"recall: \",recall)\n",
    "print(\"f1: \",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_confusionMatrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_score = clf.predict_proba(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score[:,1])\n",
    "plot_roc_curve(fpr, tpr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'y_test':y_test,'y_score[:,1]': y_score[:,1]})\n",
    "df1.to_csv('Pictures/pre_lab_8.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)\n",
    " \n",
    "clf = clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "precision, recall, f1 = precision_recall_fscore_support(y_test,y_pred,average='macro')[:-1]\n",
    "print(\"accuracy: \", accuracy)\n",
    "print(\"precision: \",precision)\n",
    "print(\"recall: \",recall)\n",
    "print(\"f1: \",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_confusionMatrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_score = clf.predict_proba(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score[:,1])\n",
    "plot_roc_curve(fpr, tpr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'y_test':y_test,'y_score[:,1]': y_score[:,1]})\n",
    "df1.to_csv('Pictures/pre_lab_9.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = clf = RandomForestClassifier(n_estimators=10, max_depth=3, min_samples_split=12, random_state=0)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "precision, recall, f1 = precision_recall_fscore_support(y_test,y_pred,average='macro')[:-1]\n",
    "print(\"accuracy: \", accuracy)\n",
    "print(\"precision: \",precision)\n",
    "print(\"recall: \",recall)\n",
    "print(\"f1: \",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_confusionMatrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_score = clf.predict_proba(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score[:,1])\n",
    "plot_roc_curve(fpr, tpr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'y_test':y_test,'y_score[:,1]': y_score[:,1]})\n",
    "df1.to_csv('Pictures/pre_lab_10.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "precision, recall, f1 = precision_recall_fscore_support(y_test,y_pred,average='macro')[:-1]\n",
    "print(\"accuracy: \", accuracy)\n",
    "print(\"precision: \",precision)\n",
    "print(\"recall: \",recall)\n",
    "print(\"f1: \",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_confusionMatrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_score = clf.predict_proba(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score[:,1])\n",
    "plot_roc_curve(fpr, tpr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'y_test':y_test,'y_score[:,1]': y_score[:,1]})\n",
    "df1.to_csv('Pictures/pre_lab_11.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "precision, recall, f1 = precision_recall_fscore_support(y_test,y_pred,average='macro')[:-1]\n",
    "print(\"accuracy: \", accuracy)\n",
    "print(\"precision: \",precision)\n",
    "print(\"recall: \",recall)\n",
    "print(\"f1: \",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_confusionMatrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_score = clf.predict_proba(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score[:,1])\n",
    "plot_roc_curve(fpr, tpr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'y_test':y_test,'y_score[:,1]': y_score[:,1]})\n",
    "df1.to_csv('Pictures/pre_lab_12.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(learning_rate=0.3, max_depth=1, random_state=0)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "precision, recall, f1 = precision_recall_fscore_support(y_test,y_pred,average='macro')[:-1]\n",
    "print(\"accuracy: \", accuracy)\n",
    "print(\"precision: \",precision)\n",
    "print(\"recall: \",recall)\n",
    "print(\"f1: \",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_confusionMatrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_score = clf.predict_proba(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score[:,1])\n",
    "plot_roc_curve(fpr, tpr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'y_test':y_test,'y_score[:,1]': y_score[:,1]})\n",
    "df1.to_csv('Pictures/pre_lab_13.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
